services:

  # ── Infrastructure ────────────────────────────────────────────────────────

  qdrant:
    image: qdrant/qdrant:v1.7.4
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_data:/qdrant/storage
    healthcheck:
      test: ["CMD-SHELL", "timeout 1 bash -c '</dev/tcp/localhost/6333' || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD-SHELL", "timeout 1 bash -c '</dev/tcp/localhost/11434' || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 10
      start_period: 30s

  ollama-init:
    image: curlimages/curl:latest
    depends_on:
      ollama:
        condition: service_healthy
    entrypoint: >
      sh -c "curl -X POST http://ollama:11434/api/pull -d '{\"name\": \"mistral:7b\"}' && echo 'Model pulled successfully'"
    restart: "no"

  # ── Nuclio Functions (AI compute) ─────────────────────────────────────────
  #
  # These follow the Nuclio function contract:
  #   init_context(context) — called once at startup, loads models
  #   handler(context, event) — called per request, stateless compute
  #
  # The runner is nuclio_runner.py (stdlib http.server, no FastAPI).

  fn-ingest:
    # Chunks text, embeds with BAAI/bge-small-en-v1.5, upserts to Qdrant
    build:
      context: functions/fn-ingest
      dockerfile: Dockerfile
    ports:
      - "9090:8080"       # host:container
    environment:
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - LOG_LEVEL=INFO
    depends_on:
      qdrant:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "timeout 1 bash -c '</dev/tcp/localhost/8080' || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s   # time for SentenceTransformer init

  fn-agent:
    # Intent classification + LlamaIndex ReActAgent + 7 career analysis tools
    build:
      context: functions/fn-agent
      dockerfile: Dockerfile
    ports:
      - "9091:8080"       # host:container
    environment:
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - OLLAMA_BASE_URL=http://ollama:11434
      - LOG_LEVEL=INFO
    depends_on:
      qdrant:
        condition: service_healthy
      ollama:
        condition: service_healthy
      ollama-init:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD-SHELL", "timeout 1 bash -c '</dev/tcp/localhost/8080' || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 10
      start_period: 120s  # time for LLM + embedding model init

  # ── Backend (API Gateway) ─────────────────────────────────────────────────
  #
  # Pure orchestration: routing, validation, document registry, PDF extraction.
  # Zero ML dependencies — sentence-transformers and torch are NOT installed.

  backend:
    build:
      context: .
      dockerfile: backend/Dockerfile
    ports:
      - "8000:8000"
    environment:
      - FN_INGEST_URL=http://fn-ingest:8080   # document ingestion function
      - NUCLIO_URL=http://fn-agent:8080        # AI agent function
      - LOG_LEVEL=INFO
    volumes:
      - upload_data:/app/uploads              # persists document_registry.json
    depends_on:
      fn-ingest:
        condition: service_healthy
      fn-agent:
        condition: service_healthy

  # ── Frontend ──────────────────────────────────────────────────────────────

  frontend:
    build:
      context: frontend
      dockerfile: Dockerfile
    ports:
      - "3000:80"
    depends_on:
      - backend

volumes:
  qdrant_data:
  ollama_data:
  upload_data:
